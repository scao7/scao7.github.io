+++
author = "Shengting Cao"
categories = ["Computer Vision"]
date = "2020-01-17"
description = "Alexnet, VGG, ResNet,inception and sliding window"
featured = ""
featuredalt = "Pic 3"
featuredpath = "date"
linktitle = ""
title = "Computer vision problem and step by step"
type = "post"
+++
This post is review some cases in computer vision problem. How all the method constructed.

#### LeNet-5

![pic11](/img/2020/01/pic06.png)
This network use 60k parameters, relative small network.

Network get deeper hight, width get smaller, channels larger.

Note: sigmoid/tanh , non-linear in avg pool, II and III of the paper.

#### AlexNet
![pic11](/img/2020/01/pic07.png)
using Relu

Multiple GPUs

local response normalization  (doesn't help much)

#### VGG -16

![pic08](/img/2020/01/pic08.png)
Uniform

large network

VGG-19 is a bigger version of network

#### ResNet
![pic09](/img/2020/01/pic09.png)
![pic10](/img/2020/01/pic10.png)


Residual blocks

skip connection

**why do residual networks work?**



#### Inception
![pic11](/img/2020/01/pic11.png)

Basically, inception just apply different filter and output a final volume with all the data and combined together.

The network can choose which size of filter they use and what kind of filter they use. The drawback is it require much more computational power.

**solve problem of computational cost**

Using a 1 * 1 convolution to reduce the volume and do further operation.

![pic11](/img/2020/01/pic12.png)

#### convolutional implementation of sliding windows

![pic13](/img/2020/01/pic13.png)

In object detection, if we want to draw a box around the object. We could do sliding window for the full image and then once we find the object we drew the box.

To implement this in a convolutional way, we just do the convolution on the big image. The final result will automatically represent each sliding window. In this case, we save a lot of time.
