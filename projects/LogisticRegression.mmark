+++
author = "Shengting Cao"
categories = ["Deep Learning"]
date = "2019-09-21"
description = "Talk about neuron network basics, logistic regress, loss funciton, cost funciton ... "
featured = ""
featuredalt = "deeplearning"
featuredpath = "date"
linktitle = ""
title = "Neuron Network Basics"
type = "post"
+++

#### What is logistic regression
**Logistic regression** is the appropriate regression analysis to conduct when the dependent variable is binary.

#### Use logistic regression in deep learning:

Given an image $$ x $$ and we want an algorithm to output **$$\hat{y}$$**, 1 represents cat and 0 represents non-cat.

Given $$x$$, want $$\hat{y} = p(y=1|x)$$ and $$ x \in \R^{\smash{n_x}} $$

Parameters: $$w \in \R^{\smash{n_x}}$$ and $$b \in \R^{\smash{n}}$$

Output: $${\hat{y} = w^Tx + b} $$ (this one is linear regression, but this is not
 a good algorithm for binary classification because you want the $$\hat{y}$$ to
 be the chance that if the image is cat. So $$\hat{y}$$ should between 0 and 1.
 And it is difficult to enforce that, $$w^Tx + b$$ can be much bigger than 1 and
 might be negative. Which doesn't make sense in probability.)

**So for the logistic regression the output should be $$\\ \hat{y} = \sigma(w^Tx + b)$$**

The sigmoid function looks like this:

<img src = "/img/2019/09/pic01.png" width = 40%/>

We use $$z$$ to denote $$ w^T + b$$ and the formula of sigmoid function:

$$ \sigma (z) = \frac 1 {1 + e^{-z}}$$

If $$z$$ is very large $$\sigma (z) $$ is closer to 1.
If $$z$$ is very small $$\sigma (z) $$ is closer to 0.

when you implement the logistic regression the $$\hat{y}$$ can be a good estimate of the chance that output is 0 or 1.

#### Cost Function VS Loss function
To learn the parameters of your model.

We given $$\{ (x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)}) \},$$ want $$ \hat{y}^{(i)} \approx y^{(i)}$$.
you have m sample training set and you want the output prediction approximately equal to the ground truth.

we can have more detail formula according to the logistic regression formulas.

 **$$\\ \hat{y}^{(i)} = \sigma(w^Tx^{(i)} + b^{(i)})\ $$, where $$\ \sigma(z^{(i)}) = \frac 1 {1 + e^{-z^{(i)}}}$$**

 Now we can have the loss function to measure how well is our estimation.

 **Loss Function**

 Intuitively you could define a function like this: $$\mathscr{L}(\hat{y},y) = \frac 1 2 (\hat{y}-y)^2$$

 **But this formula is not convex and makes gradient descent may not find a global optimal.**

 We are not simply use the mean square error function because it's a non-convex function.

 Examples of convex and non-convex function:
 ![convex and non convex](/img/2019/09/pic03.jpg)
 Our cost function is a convex one. No matter how we initialize our training it will eventually go to the same point.

 The formula we really use is:

 $$\bold{\mathscr{L}(\hat{y},y) = -(y\log \hat{y} + (1-y)\log(1-\hat{y}))}$$

 we want the loss function as small as possible. In this formula

 If $$y = 1 : \mathscr{L}(\hat{y}, y) = -\log \hat{y} \larr $$ want $$\, \log \hat{y} $$ large, want $$\hat{y}$$ large.

 If $$y = 0 : \mathscr{L}(\hat{y}, y) = -\log (1- \hat{y}) \larr $$ want $$\, \log(1-\hat{y}) $$ large, want $$ \hat{y}$$ small.

 **Loss function is define as the loss on a single training sample**

 **Cost Function**

 $$ J(w,b)= \frac 1 m \sum_{i=1}^m \mathscr{L}(\hat{y}^{(i)}, y^{(i)}) = - \frac 1 m \sum_{i=1}^m [ y^{(i)}\log \hat{y}^{(i)} + (1-y^{(i)})\log(1-\hat{y}^{(i)})]$$

 **Cost function is define as the loss of entire training samples**

 Finally we are trying to find $$w$$ and $$b$$ to minimize cost $$J(w,b)$$.

 #### Gradient Descent

Gradient Descent is a first-order iterative optimization algorithm for finding the minimum of a function.

Gradient Descent will take the next several steps to find the deepest point of the convex surface:

To illustrate how Gradient works, we can draw a graph like this (ignore the $$b$$):

<img src = "/img/2019/09/pic04.png" width = 50%\>

**Repeat {**

$$ {w:= w - \alpha \frac {\partial J(w)} {\partial w}}$$;

**}**

If the slop of this function is positive the $$w$$ will move to the left, if slop is negative the $$w$$ will move to the right. By doing this several times, we will find a global minimum of $$J(w)$$.





$$\boxed{\alpha}$$ is the learning rate and controls how big a step we take on each iteration of gradient descent.

$$\boxed{\partial w}$$ is the derivative update the change you want to make to the parameters $$w$$.

$$\boxed{\partial J(w)}$$ is the changes related to $$\partial w$$ of $$J(w)$$.

$$\boxed{\frac {\partial J(w)} {\partial w}}$$ is the slop of this curve.

#### computational graph

**Forward Propagation**

Given funciton $$J(a,b,c) = 5 a(b + c)$$ Let's say $$u = b + c, v = au, J = 5v$$

$$ \boxed{a = 5} $$ ------------------------------------- $$\searrow$$

$$ \boxed{b = 3} $$ ----------->$$ \boxed{u=bc} \longrightarrow \boxed{v = a + u} \longrightarrow \boxed{J= 3v}$$

$$ \boxed{c = 2} $$ ---------- $$\nearrow$$

In this case we calculate from left to right and it's forward propagation.

**Backward Propagation**

<img src = "/img/2019/09/pic05.png" width = "50%" />

According to chain rules for calculus

$$\frac {\partial J} {\partial a} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial a} $$
, $$\ \frac{\partial J}{\partial b}  = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial b}$$,
$$ \frac {\partial J}{\partial c} = \frac{\partial J}{\partial v} \cdot  \frac{\partial v}{\partial u} \cdot \frac{\partial u}{\partial c}$$


By using this Backward Propagation we can end up this $$ \frac {\partial FindOutputVar}{\partial Var} $$

#### logistic regression derivative
Now let's go back to the original example of the logistic regression.

<img src = "/img/2019/09/pic06.png" width = 50%/>

We can take the formula of loss function and take the derivative:

$$ \frac {\partial \mathscr{L}(a,y)}{\partial a} = -\frac{y}{a} + \frac {1-y}{1-a} $$
,$$ \frac {\partial a}{\partial z} = a(1-a) $$
$$ \Longrightarrow$$ $$ \partial z =\frac{\partial \mathscr{L}}{\partial a } \cdot \frac{\partial a}{\partial z} = \frac{ \partial \mathscr{L}}{\partial z}  =  \frac{\partial \mathscr{L}(a,y)}{\partial z} = a - y $$

Now we know $$\partial z = a - y$$. We can further know $$ \frac {\partial \mathscr{L}} {\partial w} = x_1 \cdot \partial z , \partial b = \partial z$$.

You can update $$w_1 := w_1 - \alpha \cdot \partial w_1$$ and $$ w_2 = w_2 - \alpha \cdot w_2$$.

#### Apply the logistic derivative to m training set

**Initialize the data:**

$$J = 0; \partial w_1 = 0; \partial w_2 = 0 ;\partial b = 0; \newline $$

**For i = 1 to m:**

$$ z^{(i)} = w^Tx{(i)}+b \newline a^{(i)} = \sigma(z^{(i)}) \newline J+=-[y{(i)}\log a^{(i)} +(1-y{(i)})\log(1-a{(i)})] \newline \partial z^{(i)} = a^{(i)}-y^{(i)} \newline \partial w_1 += x^{(i)}_1\partial z^{(i)} \newline \partial w_2 += x^{(i)}_2\partial z^{(i)} \newline \partial b += \partial z^{(i)}$$

**After the loop we need divide by m:**
$$\newline  J/=m , \partial w_1 / m, \partial w_2 / m, \partial b / m.$$

The same as previous example, you need update the  $$w_1 := w_1 - \alpha \cdot \partial w_1$$ and $$ w_2 := w_2 - \alpha \cdot \partial w_2, b = b - \alpha \cdot \partial b$$

This post illustrated the basic ideas of logistic regression, I will post more about code implementation later.
