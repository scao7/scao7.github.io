+++
author = "Shengting Cao"
categories = ["Probability"]
date = "2019-09-25"
description = "This post will take about the basic probability method to prepare the machine learning knowledge."
featured = ""
featuredalt = "Pic 3"
featuredpath = "date"
linktitle = ""
title = "Probability Basics"
type = "post"
+++
[Probability Documentation PDF](/img/2019/09/Probability.pdf),
[Multivariate Methods PDF](/img/2019/09/MultivariateMethod.pdf)

Recently I have been through a hard time to understand the classification methods in Machine Learning. I feel the probability basics is very important to the Machine Learning. In this post, I will write down the formula about probability calculation. I will combine them with Matlab code expression and Matrix manipulate to help me better understand those concept.
#### Bay's formula
Most frequent used theory in probability.

$$P(E|F) = \frac {P(E\cap F)} {P(E)} , P(E \cap F) = P(E|F)P(F) = P(F|E)P(E) \Longrightarrow  P(F|E)=\frac {P(E|F)P(F)}{P(E)}$$


#### Notations
$$\boxed{x}$$ Scalar value

$$\boxed{\bold{x}} $$ Vector

$$\boxed{\bold{X}} $$ Matrix

$$\boxed{\bold{x}^T}$$ Transpose

$$\boxed{X}$$ Random variable

$$\boxed{P(X)}$$ Probability mass function when $$X$$ is discrete

$$\boxed{\rho(X)}$$ Probability density function when $$X$$ is continuous

$$\boxed{P(X|Y)}$$ Conditional probability of $$X$$ given $$Y$$

$$\boxed{E[X]}$$ Expected value of the random variable $$X$$

$$\boxed{Var(X)} $$ Variance of X

$$\boxed{Cov(X,Y)}$$ Covariance of X and Y

$$\boxed{Corr(X,Y)}$$ Correlation of X and Y

$$\boxed{\mu}$$ Mean,

$$\boxed{\sigma^2}$$ Variance

$$\boxed{\varSigma}$$  Covariance matrix

$$\boxed{m}$$ Estimator to the mean

$$\boxed{s^2}$$ Estimator to the variance

$$\boxed{\bold{s}}$$ Estimator to the covariance matrix

$$\boxed{N(\mu,\sigma^2)}$$ Univariate normal distribution with mean $$\mu$$ and variance $$\sigma^2$$

$$\boxed{Z}$$ Unit normal distribution $$N(0,1)$$

$$\boxed{N_d(\bold{\mu},\bold{\varSigma)}}$$ d-variate normal distribution with mean vector $$\mu$$and covariance matrix $$\varSigma$$

--------
$$\boxed{x}$$ Input

$$\boxed{d}$$ Number of inputs (input dimensionality)

$$\boxed{\gamma}$$ Output

$$\boxed{r}$$ required output

$$\boxed{K}$$ Number of outputs (classes)

$$\boxed{N}$$ Number of training instances

$$\boxed{z}$$ Hidden value ,intrinsic dimension, latent factor

$$\boxed{k}$$ Number of hidden dimensions , latent factors

$$\boxed{C_i}$$ Class $$i$$

$$\boxed{\chi}$$ Training sample

$$\boxed{{x^t}^N_{t=1}}$$ Set of $$x$$ with index $$t$$ ranging from 1 to N

$$\boxed{{\{x^t,r^t}_t\}}$$ Set of ordered pairs of input and desired output with index t

--------
$$\boxed{g(x|\theta)}$$ Function of x defined up to a set of parameters $$\theta$$

$$\boxed{arg \ max_\theta g(x|\theta)}$$ The argument $$\theta$$ for which g has its maximum value

$$\boxed{arg \ min_\theta g(x|\theta)}$$ The argument $$\theta$$ for which $$g$$ has its minimum value

$$\boxed{E(\theta|\chi)}$$ Error function with parameters $$\theta$$ on the sample $$\chi$$

$$\boxed{l(\theta|\chi)}$$ Likelihood of parameters $$\theta$$ on the sample $$\chi$$

$$\boxed{\mathscr{L}(\theta|\chi)} $$ Log likelihood of parameters $$\theta$$ on the sample $$\chi$$

---------
$$\boxed{1(c)}$$ 1 if c is true , 0 otherwise

$$\boxed{\# \{ c\}}$$ Number of elements for which c is true

$$\boxed{\delta_{ij}}$$ Kronecker delta: 1 if i = j, 0 otherwise
